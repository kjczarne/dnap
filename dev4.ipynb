{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for DNA+P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/chris/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "memory.used [MiB]\n",
      "18 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.used --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (/home/chris/.cache/huggingface/datasets/csv/default-68212c3a0ebc43dc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1134.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 47356\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 5920\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 5920\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = dict(\n",
    "    train=\"./promoter_detection/train.csv\",\n",
    "    test=\"./promoter_detection/test.csv\",\n",
    "    val=\"./promoter_detection/dev.csv\"\n",
    ")\n",
    "\n",
    "promoter_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "promoter_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = promoter_dataset[\"train\"]\n",
    "val_dataset = promoter_dataset[\"val\"]\n",
    "test_dataset = promoter_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = train_dataset['sequence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "\n",
    "def dnap_tokenize(seq: str, stop_symbol: str = \"*\"):\n",
    "    # find boundaries of the coding sequence\n",
    "    start = seq.lower().find(\"atg\")\n",
    "    stop = seq.lower().find(\"tga\")\n",
    "    print(f\"start: {start}\")\n",
    "    print(f\"stop: {stop}\")\n",
    "    protein_seq = str(Seq(seq[start:stop]).translate())\n",
    "    return seq[0:start-1] + protein_seq + stop_symbol + seq[stop+3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 16\n",
      "stop: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages/Bio/Seq.py:2804: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TATAATAATAACGAA*GACGACAGTCGACAAGAAAAGCACCAGCTGTCCCCGCCACATACAAGTATATGAGAAGGGGACGCGGGAGAGCGCCGCGGGGGACCGACGCGCTATTGAGGGGGATGGGTACAAGCGGGGCGGGGAGGCCGGAGCTTTATCCAGGCCAATGAATGGCCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAG'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnap_tokenize(seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive tokenizer like this does not account for ORF-sensitivity and will simply find the first occurence of the stop codon, even if it follows the start codon right away. All sequences such as `atga` would then be truncated to a stop codon with Biopython translation API. So we need something smarter that chunks up the sequences into codons once the ORF is found. This is a good time to develop or pick up an ORF finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/chris/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: orffinder in /home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages (1.8)\n",
      "Requirement already satisfied: biopython>=1.79 in /home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages (from orffinder) (1.81)\n",
      "Requirement already satisfied: numpy in /home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages (from biopython>=1.79->orffinder) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install orffinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 17, 'end': 173, 'frame': 2, 'sense': '+', 'length': 156, 'trailing': False, 'index': 1}, {'start': 173, 'end': 300, 'frame': 2, 'sense': '+', 'length': 127, 'trailing': True, 'index': 2}, {'start': 186, 'end': 300, 'frame': 3, 'sense': '+', 'length': 114, 'trailing': True, 'index': 3}, {'start': 169, 'end': 259, 'frame': 1, 'sense': '+', 'length': 90, 'trailing': False, 'index': 4}, {'start': 63, 'end': 1, 'frame': 2, 'sense': '-', 'length': 61, 'trailing': True, 'index': 5}, {'start': 259, 'end': 300, 'frame': 1, 'sense': '+', 'length': 41, 'trailing': True, 'index': 6}]\n"
     ]
    }
   ],
   "source": [
    "from Bio.SeqIO import SeqRecord\n",
    "from orffinder import orffinder\n",
    "\n",
    "orfs = orffinder.getORFs(SeqRecord(Seq(train_dataset['sequence'][0])), minimum_length=3)\n",
    "print(orfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orf = orfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 17,\n",
       " 'end': 173,\n",
       " 'frame': 2,\n",
       " 'sense': '+',\n",
       " 'length': 156,\n",
       " 'trailing': False,\n",
       " 'index': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def dnap_tokenize(seq: str, orf_data: Dict[str, Any], stop_symbol: str = \"*\"):\n",
    "    start = orf_data['start']\n",
    "    stop = orf_data['end']\n",
    "    protein_seq = str(Seq(seq[start:stop]).translate())\n",
    "    return seq[0:start-1] + protein_seq + stop_symbol + seq[stop+3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TATAATAATAACGAAG*DDSRQEKHQLSPPHTSI*EGDAGERRGGPTRY*GGWVQAGRGGRSFIQANE*CCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAG'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnap_tokenize(seq, orf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's cool and all but these are all promoter sequences, so within those 300 bp here we would not expect any actual genes to be found.\n",
    "\n",
    "What we could do instead is append those sequences to existing gene sequences and encode specifically those gene sequences only using a DNA+P representation. We would be using synthetic data but it would be much easier to prepare than looking for specific genes in databases that are prepended by those promoters that Prom300 contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TATAATAATAACGAAGATGAGACGACAGTCGACAAGAAAAGCACCAGCTGTCCCCGCCACATACAAGTATATGAGAAGGGGACGCGGGAGAGCGCCGCGGGGGACCGACGCGCTATTGAGGGGGATGGGTACAAGCGGGGCGGGGAGGCCGGAGCTTTATCCAGGCCAATGAATGGCCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAG'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's develop a tiny function that will append randomly generated gene sequences to the sequences in the dataset that are marked as promoters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TATAATAATAACGAAGATGAGACGACAGTCGACAAGAAAAGCACCAGCTGTCCCCGCCACATACAAGTATATGAGAAGGGGACGCGGGAGAGCGCCGCGGGGGACCGACGCGCTATTGAGGGGGATGGGTACAAGCGGGGCGGGGAGGCCGGAGCTTTATCCAGGCCAATGAATGGCCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAGGCAGGCAAGTGGGGCACCCGTATCCTTTCCAACTTACAAGGGTCCCCGTTGTGCGCCAGAGGAAGTCACTTTATATCCGCGCACGGTACT'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from typing import Dict, Any, List\n",
    "import Bio.Data.CodonTable as CodonTable\n",
    "\n",
    "\n",
    "def append_gene_to_promoter(seq: str,\n",
    "                            gene_length: int,\n",
    "                            is_promoter_seq: bool = True,\n",
    "                            seed: int = 42,\n",
    "                            codon_table: CodonTable.CodonTable = CodonTable.standard_dna_table):\n",
    "    np.random.seed(seed)\n",
    "    num_codons = gene_length - 6 // 3  # -6 because we account for 1 start and 1 stop codon per gene appended\n",
    "    actual_gene_length = num_codons * 3  # recover if `gene_length` was not divisible by 3\n",
    "    if is_promoter_seq:\n",
    "        # 1. Generate a random in-ORF gene of length `gene_length`:\n",
    "        start = codon_table.start_codons[0]  # ATG\n",
    "        stop = np.random.choice(codon_table.stop_codons)  # randomly choose one\n",
    "        codon_seq = np.random.choice(list(codon_table.forward_table.keys()),\n",
    "                                     size=num_codons,\n",
    "                                     replace=True)\n",
    "        codon_seq = \"\".join(list(codon_seq))\n",
    "        # 2. Append the gene to the promoter sequence if the sequence is marked as a promoter:\n",
    "        return seq + start + codon_seq + stop\n",
    "    else:\n",
    "        # 1. Generate a random DNA sequence that does not start with an `ATG` codon\n",
    "        nucleotides = [\"A\", \"T\", \"G\", \"C\"]\n",
    "        rand_nucl_seq = np.random.choice(nucleotides,\n",
    "                                         size=actual_gene_length + 6,  # +6 because we account for the START/STOP\n",
    "                                                                       # codons that are missing here\n",
    "                                         replace=True)\n",
    "        # 2. Append the padding sequence to the non-promoter sequence\n",
    "        rand_nucl_seq = \"\".join(list(rand_nucl_seq))\n",
    "        return seq + rand_nucl_seq\n",
    "\n",
    "\n",
    "def append_gene_to_dataset_record(seq: str, label: str, gene_length: int, seed: int = 42):\n",
    "    match label:\n",
    "        case 0:\n",
    "            return dict(sequence=append_gene_to_promoter(seq, gene_length, False, seed), label=label)\n",
    "        case 1:\n",
    "            return dict(sequence=append_gene_to_promoter(seq, gene_length, True, seed), label=label)\n",
    "        case _:\n",
    "            raise ValueError(f\"{label} is not a valid label for a binary classification task\")\n",
    "\n",
    "\n",
    "def append_gene_to_dataset_batch(batch: Dict[str, List[Any]], gene_length: int, seed: int = 42):\n",
    "    seqs = batch[\"sequence\"]\n",
    "    labels = batch[\"label\"]\n",
    "    out = dict(sequence=[], label=[])\n",
    "    for seq, label in zip(seqs, labels):\n",
    "        updated_record = append_gene_to_dataset_record(seq, label, gene_length, seed)\n",
    "        out[\"sequence\"].append(updated_record[\"sequence\"])\n",
    "        out[\"label\"].append(updated_record[\"label\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "def append_genes_to_dataset(dataset_collection: DatasetDict, gene_length: int, seed: int = 42):\n",
    "    new_dataset_collection = DatasetDict()\n",
    "    for k in dataset_collection.keys():\n",
    "        # `batched=True` speeds things up by processing sequences in batches\n",
    "        new_dataset_collection[k] = dataset_collection[k].map(lambda batch: append_gene_to_dataset_batch(batch,\n",
    "                                                                                                         gene_length,\n",
    "                                                                                                         seed),\n",
    "                                                              batched=True)\n",
    "    return new_dataset_collection\n",
    "\n",
    "\n",
    "append_gene_to_promoter(seq, 30, True)\n",
    "append_gene_to_promoter(seq, 30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_with_gene = append_gene_to_promoter(seq, 30, is_promoter_seq=True)\n",
    "seq_with_rand = append_gene_to_promoter(seq, 30, is_promoter_seq=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chris/.cache/huggingface/datasets/csv/default-68212c3a0ebc43dc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-8241f620c99d71f2.arrow\n",
      "Loading cached processed dataset at /home/chris/.cache/huggingface/datasets/csv/default-68212c3a0ebc43dc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-66cd6d234ba3f96e.arrow\n",
      "Loading cached processed dataset at /home/chris/.cache/huggingface/datasets/csv/default-68212c3a0ebc43dc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-1177803720104f82.arrow\n"
     ]
    }
   ],
   "source": [
    "promoter_dataset_with_genes_appended = append_genes_to_dataset(promoter_dataset, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 47356\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 5920\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 5920\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promoter_dataset_with_genes_appended"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: because each sequence in the Prom300 dataset was exactly 300bp-long, we now know that ORFs for those samples that _were_ marked as actual promoters are in fact fake genes that we appended. We can **prepare a tokenizer for DNA+P representation with this in mind** considering only the protein-encoding part (over the 300bp boundary) as our starting point for the ORF. So we need to consider two things for the tokenizer:\n",
    "1. Start translating the ORF at 301bp mark (that will be string idx `300`).\n",
    "2. Only the sequences labeled as containing promoters should have the tokenizer applied to them.\n",
    "\n",
    "**Very important**: for the test set the DNA+P tokenizer **must not** convert the sequences to DNA+P representation. They should stay in their original form so that we can check whether _the model_ learned to pick up on those promoter sequences better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<CLS>', '<ACGTGT>', '<A>', '<C>', '<N>', '<TGCACG>', '<G>', '<A>', '<N>', '<CGACTA>', '<GTCTGA>']\n",
      "['<CLS>', '<ACGTGT>', '<A>', '<C>', '<N>', '<TGCACG>', '<G>', '<A>', '<N>', '<CGACTA>', '<GTCTGA>']\n",
      "True\n",
      "['<CLS>', ('<ACGTGT>', 0, 6), ('<A>', 0, 1), ('<C>', 1, 2), ('<N>', 2, 3), ('<TGCACG>', 9, 15), ('<G>', 0, 1), ('<A>', 1, 2), ('<N>', 2, 3), ('<CGACTA>', 18, 24), ('<GTCTGA>', 24, 30), ('<MetAsp>', 30, 36), ('<AspAsp>', 36, 42), ('<Asp*>', 42, 48)]\n",
      "['<CLS>', ('<ACGTGT>', 0, 6), ('<A>', 0, 1), ('<C>', 1, 2), ('<N>', 2, 3), ('<TGCACG>', 9, 15), ('<G>', 0, 1), ('<A>', 1, 2), ('<N>', 2, 3), ('<CGACTA>', 18, 24), ('<GTCTGA>', 24, 30), ('<MetAsp>', 30, 36), ('<AspAsp>', 36, 42), ('<Asp*>', 42, 48)]\n",
      "True\n",
      "('<TATAATAATAACGAAGATGAGACGACAGTCGACAAGAAAAGCACCAGCTGTCCCCGCCACATACAAGTATATGAGAAGGGGACGCGGGAGAGCGCCGCGGGGGACCGACGCGCTATTGAGGGGGATGGGTACAAGCGGGGCGGGGAGGCCGGAGCTTTATCCAGGCCAATGAATGGCCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAG>', -1, -1)\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    NormalizedString,\n",
    "    PreTokenizedString\n",
    ")\n",
    "from itertools import accumulate, islice\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Literal, Optional\n",
    "from Bio.Data.CodonTable import standard_dna_table\n",
    "\n",
    "protein_alphabet_map = {\n",
    "    \"A\": \"Ala\",\n",
    "    \"C\": \"Cys\",\n",
    "    \"D\": \"Asp\",\n",
    "    \"E\": \"Glu\",\n",
    "    \"F\": \"Phe\",\n",
    "    \"G\": \"Gly\",\n",
    "    \"H\": \"His\",\n",
    "    \"I\": \"Ile\",\n",
    "    \"K\": \"Lys\",\n",
    "    \"L\": \"Leu\",\n",
    "    \"M\": \"Met\",\n",
    "    \"N\": \"Asp\",\n",
    "    \"P\": \"Pro\",\n",
    "    \"Q\": \"Gln\",\n",
    "    \"R\": \"Arg\",\n",
    "    \"S\": \"Ser\",\n",
    "    \"T\": \"Thr\",\n",
    "    \"V\": \"Val\",\n",
    "    \"W\": \"Trp\",\n",
    "    \"Y\": \"Tyr\",\n",
    "    \"*\": \"*\"\n",
    "}\n",
    "\n",
    "\n",
    "class Uppercase:\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def normalize(self, normalized: NormalizedString):\n",
    "        return normalized.uppercase()\n",
    "\n",
    "    def normalize_str(self, sequence: str):\n",
    "        return sequence.upper()\n",
    "\n",
    "\n",
    "def take_n(iterable: Iterable, n: int):\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        chunk = \"\".join(list(islice(iterator, n)))\n",
    "        if chunk is None:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "@dataclass\n",
    "class DNAPConfig:\n",
    "    start_translating_from_idx: int\n",
    "    _length: int = -1\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        # if self._length == -1:\n",
    "            # return -1\n",
    "        return self._length\n",
    "\n",
    "\n",
    "    def translate(self, seq: str):\n",
    "        return dnap_tokenize(seq, dict(start=self.start_translating_from_idx,\n",
    "                                       end=self.start_translating_from_idx + self.length))\n",
    "\n",
    "\n",
    "def _wrap_in_angle_brackets(seq: str, wrap_tokens: bool = True):\n",
    "    return seq if not wrap_tokens else \"<\" + seq + \">\"\n",
    "\n",
    "\n",
    "def _pre_tokenize(sequence: str,\n",
    "                  k: int,\n",
    "                  include_cls: bool,\n",
    "                  split_on_unknown_base: bool,\n",
    "                  wrap_tokens: bool,\n",
    "                  include_ranges: bool,\n",
    "                  representation: Literal[\"dna\", \"dnap\"],\n",
    "                  dnap_config: Optional[DNAPConfig] = None):\n",
    "    \n",
    "    def _with_ranges(seq: str, idx_start: int, idx_end: int):\n",
    "        subslice = seq\n",
    "        if include_ranges:\n",
    "            subslice = (seq, idx_start, idx_end)\n",
    "        return subslice\n",
    "\n",
    "    def _dnap_chunk_tokenize(seq: str, stop_symbol: str = \"*\"):\n",
    "        out = []\n",
    "        n = 3\n",
    "        chunks = [seq[i:i+n] for i in range(0, len(seq), n)]\n",
    "        lookup_table = deepcopy(standard_dna_table.forward_table)\n",
    "        lookup_table.update({k: stop_symbol for k in standard_dna_table.stop_codons})\n",
    "        # Replace single-letter identifiers with the 3-letter ones:\n",
    "        three_letter_lookup_table = {k: protein_alphabet_map[v] for k, v in lookup_table.items()}\n",
    "        for codon in chunks:\n",
    "            try:\n",
    "                aa = three_letter_lookup_table[codon]\n",
    "                out.append(aa)\n",
    "            except KeyError:\n",
    "                out.append(codon)\n",
    "        return \"\".join(out)\n",
    "\n",
    "    if dnap_config is None and representation == \"dnap\":\n",
    "        raise ValueError(\"Missing DNAP representation config!\")\n",
    "\n",
    "    i = 0\n",
    "    j = k\n",
    "    n = len(str(sequence))\n",
    "    slices = [] if not include_cls else [\"<CLS>\"]\n",
    "    # for seq_slice in take_n(sequence, k):\n",
    "    if k == 1:\n",
    "        # slightly different logic for a 1-mer tokenizer:\n",
    "        if representation == \"dnap\":\n",
    "            idx = dnap_config.start_translating_from_idx\n",
    "            dna = seq[:idx]\n",
    "            dnap = _dnap_chunk_tokenize(seq[idx:])\n",
    "            # Note: I'm not providing valid ranges here, there's no time, need to write the paper!\n",
    "            return _with_ranges(_wrap_in_angle_brackets(dna + dnap, wrap_tokens), -1, -1)\n",
    "        else:\n",
    "            return _with_ranges(_wrap_in_angle_brackets(seq, wrap_tokens), -1, -1)\n",
    "        \n",
    "    while j <= n:\n",
    "        seq_slice = sequence[i:j]\n",
    "        if \"N\" in seq_slice and split_on_unknown_base:\n",
    "            idx_of_n = seq_slice.find(\"N\")\n",
    "            l = 0\n",
    "            subslices = []\n",
    "            while l != idx_of_n:\n",
    "                subslice = _with_ranges(_wrap_in_angle_brackets(seq_slice[l], wrap_tokens), l, l + 1)\n",
    "                subslices.append(subslice)  # split preceding into single bases\n",
    "                l += 1\n",
    "            subslice = _with_ranges(_wrap_in_angle_brackets(seq_slice[idx_of_n], wrap_tokens), idx_of_n, idx_of_n + 1)\n",
    "            subslices.append(subslice)  # add `N` itself\n",
    "            slices.extend(subslices)  # add the split-up slices to the main list\n",
    "            i = i + k - idx_of_n - 1\n",
    "            j = j + k - idx_of_n - 1\n",
    "        else:\n",
    "            if representation == \"dnap\" and i >= dnap_config.start_translating_from_idx:\n",
    "                slice_ = _with_ranges(_wrap_in_angle_brackets(_dnap_chunk_tokenize(seq_slice), wrap_tokens), i, j)\n",
    "            else:\n",
    "                slice_ = _with_ranges(_wrap_in_angle_brackets(seq_slice, wrap_tokens), i, j)\n",
    "            slices.append(slice_)\n",
    "            i += k\n",
    "            j += k\n",
    "    return slices\n",
    "\n",
    "\n",
    "# We use identical tokenization strategy to Nucleotide Transformer\n",
    "class KMerDNAPreTokenizer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 k: int = 6,\n",
    "                 include_cls: bool = True,\n",
    "                 wrap_tokens: bool = True,\n",
    "                 split_on_unknown_base: bool = True,\n",
    "                 include_ranges: bool = True) -> None:\n",
    "        \"\"\"Args:\n",
    "            `k` (int): how many bases per token\n",
    "            `include_cls` (bool): whether to include the starting `<CLS>` token or not\n",
    "            `wrap_tokens` (bool): whether to wrap each token in `<>` angle brackets\n",
    "            `split_on_unknown_base` (bool): if `True` then each `N` in the sequence will\n",
    "                                            be treated as a separate token and all preceding\n",
    "                                            bases that don't align to `k` are split up as\n",
    "                                            separate tokens\n",
    "            `include_ranges` (bool): whether token slice ranges should be returned in tuples\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.include_cls = include_cls\n",
    "        self.wrap_tokens = wrap_tokens\n",
    "        self.split_on_unknown_base = split_on_unknown_base\n",
    "        self.include_ranges = include_ranges\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        # Note: `pre_tokenize` acts in-place on `pretok`, so we should not return anything here\n",
    "        pretok.split(lambda idx, x: _pre_tokenize(x,\n",
    "                                                  self.k,\n",
    "                                                  self.include_cls,\n",
    "                                                  self.split_on_unknown_base,\n",
    "                                                  self.wrap_tokens,\n",
    "                                                  self.include_ranges,\n",
    "                                                  representation=\"dna\"))\n",
    "\n",
    "    def pre_tokenize_str(self, sequence: str):\n",
    "        return _pre_tokenize(sequence,\n",
    "                             self.k,\n",
    "                             self.include_cls,\n",
    "                             self.split_on_unknown_base,\n",
    "                             self.wrap_tokens,\n",
    "                             self.include_ranges,\n",
    "                             representation=\"dna\")\n",
    "\n",
    "\n",
    "class KMerDNAPPreTokenizer(KMerDNAPreTokenizer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 k: int = 6,\n",
    "                 include_cls: bool = True,\n",
    "                 wrap_tokens: bool = True,\n",
    "                 split_on_unknown_base: bool = True,\n",
    "                 include_ranges: bool = True,\n",
    "                 start_idx: int = 301) -> None:\n",
    "        self.start_idx = start_idx\n",
    "        super().__init__(k,\n",
    "                         include_cls,\n",
    "                         wrap_tokens,\n",
    "                         split_on_unknown_base,\n",
    "                         include_ranges)\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(lambda idx, x: _pre_tokenize(x,\n",
    "                                                  self.k,\n",
    "                                                  self.include_cls,\n",
    "                                                  self.split_on_unknown_base,\n",
    "                                                  self.wrap_tokens,\n",
    "                                                  self.include_ranges,\n",
    "                                                  representation=\"dnap\",\n",
    "                                                  dnap_config=DNAPConfig(self.start_idx)))\n",
    "\n",
    "    def pre_tokenize_str(self, sequence: str):\n",
    "        return _pre_tokenize(sequence,\n",
    "                             self.k,\n",
    "                             self.include_cls,\n",
    "                             self.split_on_unknown_base,\n",
    "                             self.wrap_tokens,\n",
    "                             self.include_ranges,\n",
    "                             representation=\"dnap\",\n",
    "                             dnap_config=DNAPConfig(self.start_idx))\n",
    "\n",
    "\n",
    "# normalizer = normalizers.Lowercase()\n",
    "# normalizer = normalizers.Normalizer.custom(Uppercase())\n",
    "# pre_tokenizer = pre_tokenizers.PreTokenizer.custom(KMerDNAPreTokenizer(6))\n",
    "# tokenizer = Tokenizer()\n",
    "# normalizer = Uppercase()\n",
    "pre_tokenizer = KMerDNAPreTokenizer(6, include_ranges=False)\n",
    "\n",
    "# we're using the same example as the Nucleotide Transformer to test:\n",
    "original = \"ACGTGTACNTGCACGGANCGACTAGTCTGA\"\n",
    "actual = pre_tokenizer.pre_tokenize_str(original)\n",
    "expected = [\"<CLS>\",\"<ACGTGT>\",\"<A>\",\"<C>\",\"<N>\",\"<TGCACG>\",\"<G>\",\"<A>\",\"<N>\",\"<CGACTA>\",\"<GTCTGA>\"]\n",
    "print(actual)\n",
    "print(expected)\n",
    "\n",
    "# compare the two lists:\n",
    "print(all(accumulate(map(lambda x: x[0] == x[1], zip(actual, expected, strict=True)), lambda x, y: x and y)))\n",
    "\n",
    "# then instantiate the other pre-tokenizer:\n",
    "pre_tokenizer_dnap = KMerDNAPPreTokenizer(6, start_idx=30)\n",
    "original = \"ACGTGTACNTGCACGGANCGACTAGTCTGAATGGATGATGATGATTGA\"\n",
    "actual = pre_tokenizer_dnap.pre_tokenize_str(original)\n",
    "expected = ['<CLS>', ('<ACGTGT>', 0, 6), ('<A>', 0, 1), ('<C>', 1, 2), ('<N>', 2, 3), ('<TGCACG>', 9, 15), ('<G>', 0, 1), ('<A>', 1, 2), ('<N>', 2, 3), ('<CGACTA>', 18, 24), ('<GTCTGA>', 24, 30), ('<MetAsp>', 30, 36), ('<AspAsp>', 36, 42), ('<Asp*>', 42, 48)]\n",
    "print(actual)\n",
    "print(expected)\n",
    "print(all(accumulate(map(lambda x: x[0] == x[1], zip(actual, expected, strict=True)), lambda x, y: x and y)))\n",
    "\n",
    "one_mer_pre_tokenizer = KMerDNAPreTokenizer(1)\n",
    "actual = one_mer_pre_tokenizer.pre_tokenize_str(original)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMerTokenizer():\n",
    "\n",
    "    def __init__(self, pre_tokenizer) -> None:\n",
    "        self.pre_tokenizer = pre_tokenizer\n",
    "        self.vocab = dict()\n",
    "\n",
    "    @property\n",
    "    def last_token_id(self):\n",
    "        all_id_vals = list(self.vocab.values())\n",
    "        if len(all_id_vals) > 0:\n",
    "            return max(all_id_vals)\n",
    "        return -1\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(list(self.vocab.values()))\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        # padding is not really a concern for our task, since all sequences are aligned to 390 bp\n",
    "        return \"<pad>\"\n",
    "\n",
    "    def tokenize(self, seq: str):\n",
    "        pre_tokenized_seq = self.pre_tokenizer.pre_tokenize_str(seq)\n",
    "        tokenized_seq = []\n",
    "        for token_tuple_or_token in pre_tokenized_seq:\n",
    "            if type(token_tuple_or_token) is tuple:\n",
    "                token, _, _ = token_tuple_or_token\n",
    "            else:\n",
    "                token = token_tuple_or_token\n",
    "            if token not in self.vocab.keys():\n",
    "                # each k-mer gets its own input token id\n",
    "                # this is the same way Nucleotide transformer does it\n",
    "                self.vocab[token] = self.last_token_id + 1\n",
    "            token_id = self.vocab[token]\n",
    "            tokenized_seq.append(token_id)\n",
    "        return tokenized_seq\n",
    "\n",
    "    def encode(self, seq: str):\n",
    "        return self.tokenize(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 2, 4, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = KMerTokenizer(pre_tokenizer)\n",
    "\n",
    "pre_tokenizer_dnap = KMerDNAPPreTokenizer(6, start_idx=301)\n",
    "pre_tokenizer = KMerDNAPreTokenizer(6, include_ranges=False)\n",
    "# pre_tokenizer_dnap = KMerDNAPPreTokenizer(3, start_idx=301)\n",
    "# pre_tokenizer = KMerDNAPreTokenizer(3, include_ranges=False)\n",
    "tokenizer = KMerTokenizer(pre_tokenizer)\n",
    "# tokenizer = KMerTokenizer(pre_tokenizer_dnap)\n",
    "actual = tokenizer.tokenize(original)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = promoter_dataset_with_genes_appended[\"train\"]\n",
    "val_dataset = promoter_dataset_with_genes_appended[\"val\"]\n",
    "test_dataset = promoter_dataset_with_genes_appended[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 11:34:08.413370: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-02 11:34:08.440715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 11:34:08.873181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from tokenizers.models import Unigram\n",
    "from pathlib import Path\n",
    "\n",
    "def model_loader(model_name: str, config, ignore_mismatched_sizes: bool = False):\n",
    "    config.num_labels=2 \n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                              config=config,\n",
    "                                                              ignore_mismatched_sizes=ignore_mismatched_sizes)\n",
    "\n",
    "model_save_dir = Path(\"results/model_nucleotide_transformer\")\n",
    "training_args = TrainingArguments(model_save_dir,\n",
    "                                  evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sequence', 'label'],\n",
       "    num_rows: 47356\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'TATAATAATAACGAAGATGAGACGACAGTCGACAAGAAAAGCACCAGCTGTCCCCGCCACATACAAGTATATGAGAAGGGGACGCGGGAGAGCGCCGCGGGGGACCGACGCGCTATTGAGGGGGATGGGTACAAGCGGGGCGGGGAGGCCGGAGCTTTATCCAGGCCAATGAATGGCCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAGGCAGGCAAGTGGGGCACCCGTATCCTTTCCAACTTACAAGGGTCCCCGTTGTGCGCCAGAGGAAGTCACTTTATATCCGCGCACGGTACT',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get an accurate estimate of the vocabulary size, we might as well produce the `input_ids` now and use them in the training loop without calling the `tokenize` method again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = []\n",
    "for record in train_dataset:\n",
    "    sequence = record[\"sequence\"]\n",
    "    label = record[\"label\"]\n",
    "    # Convert the tokens into numerical IDs using the model's vocabulary\n",
    "    input_ids = tokenizer.tokenize(sequence)\n",
    "    tokenized_train_dataset.append((input_ids, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4101"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, AdamW, EsmConfig\n",
    "import torch\n",
    "\n",
    "# model_id = \"zhihan1996/DNA_bert_6\"\n",
    "# model_id = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "model_id = \"model.bin\"\n",
    "# model_id = \"model2.bin\"\n",
    "config = EsmConfig.from_pretrained(model_id)\n",
    "# config = BertConfig.from_pretrained(model_id)\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "model = model_loader(model_id, config=config, ignore_mismatched_sizes=True)\n",
    "# model = model_loader(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\")\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4101"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    for input_ids, label in tokenized_train_dataset:\n",
    "\n",
    "        # Convert the input IDs and label into tensors\n",
    "        input_tensor = torch.tensor([input_ids])\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        label_tensor = torch.tensor([label])\n",
    "        label_tensor = label_tensor.to(device)\n",
    "        \n",
    "        # Compute the loss and perform a backward pass\n",
    "        loss = model(input_tensor, labels=label_tensor).loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model.bin\")\n",
    "# model.save_pretrained(\"model2.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/chris/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "memory.used [MiB]\n",
      "14230 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.used --format=csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing workflow adapted from the baseline notebook we used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Metrics averaged over batches: {'accuracy_score': 0.5033783783783784, \"\n",
      " \"'precision_score': 0.5033783783783784, 'recall_score': 1.0}\")\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "from typing import Callable, Tuple, Union, Iterable, List\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "\n",
    "# Splitting the test set into batches to avoid OOM errors with my lovely RTX 4080:\n",
    "# 5920 / 16 = 370\n",
    "\n",
    "Metric = Callable[[torch.Tensor | np.ndarray, torch.Tensor | np.ndarray], torch.Tensor | np.ndarray]\n",
    "\n",
    "def _eval(model: nn.Module, test_batch: torch.Tensor, attention_mask: torch.Tensor, output_hidden_states: bool):\n",
    "    with torch.no_grad():\n",
    "        torch_outs = model(\n",
    "            test_batch,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "    return torch_outs\n",
    "\n",
    "\n",
    "def test_and_calculate_metrics(tokens_ids: torch.Tensor,\n",
    "                               labels: torch.Tensor,\n",
    "                               model: nn.Module,\n",
    "                               metrics: List[Metric] | Metric,\n",
    "                               split_into: int = 4,\n",
    "                               output_hidden_states: bool = False):\n",
    "    metric_vals = dict()\n",
    "\n",
    "    if not type(metrics) is list:\n",
    "        # wrap singular metric into a list to use the same interface downstream\n",
    "        metrics = [metric]\n",
    "\n",
    "    for metric in metrics:\n",
    "        # initialize lists where we will store collected metrics\n",
    "        metric_vals[metric.__name__] = []\n",
    "\n",
    "    slice_size = tokens_ids.shape[0] // split_into\n",
    "    for test_batch, batch_labels in zip(tokens_ids.split(slice_size),\n",
    "                                        torch.tensor(labels).split(slice_size)):\n",
    "\n",
    "        # Compute the embeddings:\n",
    "        # attention_mask = test_batch != tokenizer.pad_token_id\n",
    "        attention_mask = torch.ones_like(test_batch)\n",
    "\n",
    "        # Send tokens and attention mask to the GPU:\n",
    "        test_batch = test_batch.to(\"cuda\")\n",
    "        # attention_mask = attention_mask.to(\"cuda\")\n",
    "\n",
    "        # Model outputs:\n",
    "        torch_outs = _eval(model,\n",
    "                           test_batch,\n",
    "                           attention_mask,\n",
    "                           output_hidden_states)\n",
    "        \n",
    "        y_hat_prob = nn.Sigmoid()(torch_outs.logits)\n",
    "        y_hat = torch.argmax(y_hat_prob, axis=-1)\n",
    "        for metric in metrics:\n",
    "            metric_value = metric(batch_labels.to(\"cpu\").detach().numpy(), y_hat.to(\"cpu\").detach().numpy())\n",
    "            metric_vals[metric.__name__].append(metric_value)\n",
    "\n",
    "    return metric_vals\n",
    "\n",
    "\n",
    "def testing_pipeline(tokenizer, model, test_dataset):\n",
    "\n",
    "    split_into = 4\n",
    "\n",
    "    tokens_ids = []\n",
    "    for record in test_dataset:\n",
    "        tokens_ids_per_record = tokenizer.tokenize(record[\"sequence\"])\n",
    "        tokens_ids.append(tokens_ids_per_record)\n",
    "    tokens_ids = torch.tensor(tokens_ids)\n",
    "    tokens_ids = tokens_ids.to(device)\n",
    "\n",
    "    metrics = [accuracy_score, precision_score, recall_score]\n",
    "\n",
    "    metric_vals = test_and_calculate_metrics(tokens_ids,\n",
    "                                             test_dataset[\"label\"],\n",
    "                                             model,\n",
    "                                             metrics,\n",
    "                                             split_into)\n",
    "\n",
    "    avgs_of_metric_vals = dict_average(metric_vals)\n",
    "\n",
    "    pprint(f\"Metrics averaged over batches: {avgs_of_metric_vals}\", indent=4)\n",
    "\n",
    "\n",
    "def dict_average(dict_of_metrics: Dict[str, np.array | List[int] | List[float]]) -> Dict[str, np.ndarray]:\n",
    "    avg_dict = dict()\n",
    "    for k, v in dict_of_metrics.items():\n",
    "        avg_acc = np.mean(v)\n",
    "        avg_dict[k] = avg_acc\n",
    "    return avg_dict\n",
    "\n",
    "k_mer_tokenizer = KMerTokenizer(pre_tokenizer)\n",
    "testing_pipeline(k_mer_tokenizer, model, test_dataset)\n",
    "# testing_pipeline(tokenizer, model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<CLS>', ('<CTAAAT>', 0, 6), ('<ATTAAC>', 6, 12), ('<TGGTCT>', 12, 18), ('<TGTGAG>', 18, 24), ('<ATGTCT>', 24, 30), ('<SerTrp>', 30, 36), ('<LeuGlu>', 36, 42), ('<ProAsp>', 42, 48), ('<HisLeu>', 48, 54), ('<ThrTyr>', 54, 60), ('<CysPhe>', 60, 66), ('<SerSer>', 66, 72), ('<AspCys>', 72, 78), ('<CysLeu>', 78, 84), ('<LeuLeu>', 84, 90), ('<SerLeu>', 90, 96), ('<CysCys>', 96, 102), ('<ArgLeu>', 102, 108), ('<GluLeu>', 108, 114), ('<ArgAla>', 114, 120), ('<AlaGly>', 120, 126), ('<GlyGly>', 126, 132), ('<GlyArg>', 132, 138), ('<ArgLys>', 138, 144), ('<GluGlu>', 144, 150), ('<LysGln>', 150, 156), ('<SerTrp>', 156, 162), ('<ProGly>', 162, 168), ('<SerCys>', 168, 174), ('<TrpLeu>', 174, 180), ('<GlyAla>', 180, 186), ('<ArgThr>', 186, 192), ('<AlaSer>', 192, 198), ('<LeuAsp>', 198, 204), ('<LysGln>', 204, 210), ('<AlaGly>', 210, 216), ('<AlaHis>', 216, 222), ('<IleAla>', 222, 228), ('<LeuGly>', 228, 234), ('<*Val>', 234, 240), ('<ValAla>', 240, 246), ('<LeuThr>', 246, 252), ('<HisLeu>', 252, 258), ('<AlaThr>', 258, 264), ('<AlaGly>', 264, 270), ('<ArgArg>', 270, 276), ('<GlyAla>', 276, 282), ('<ArgLys>', 282, 288), ('<ThrLeu>', 288, 294), ('<AlaLeu>', 294, 300), ('<LeuAla>', 300, 306), ('<ArgLeu>', 306, 312), ('<SerSer>', 312, 318), ('<GlyPro>', 318, 324), ('<AspGly>', 324, 330), ('<ProHis>', 330, 336), ('<CysCys>', 336, 342), ('<GlnAla>', 342, 348), ('<ThrLys>', 348, 354), ('<GlnLeu>', 354, 360), ('<HisAla>', 360, 366), ('<PheGln>', 366, 372), ('<ArgIle>', 372, 378), ('<AspPhe>', 378, 384), ('<Gly*>', 384, 390)]\n"
     ]
    }
   ],
   "source": [
    "actual = pre_tokenizer_dnap.pre_tokenize_str(test_dataset[0][\"sequence\"])\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
