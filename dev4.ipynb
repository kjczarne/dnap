{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for DNA+P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (/home/chris/.cache/huggingface/datasets/csv/default-68212c3a0ebc43dc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 3/3 [00:00<00:00, 760.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 47356\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 5920\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 5920\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = dict(\n",
    "    train=\"./promoter_detection/train.csv\",\n",
    "    test=\"./promoter_detection/test.csv\",\n",
    "    val=\"./promoter_detection/dev.csv\"\n",
    ")\n",
    "\n",
    "promoter_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "promoter_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = promoter_dataset[\"train\"]\n",
    "val_dataset = promoter_dataset[\"val\"]\n",
    "test_dataset = promoter_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = train_dataset['sequence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "\n",
    "def dnap_tokenize(seq: str, stop_symbol: str = \"*\"):\n",
    "    # find boundaries of the coding sequence\n",
    "    start = seq.lower().find(\"atg\")\n",
    "    stop = seq.lower().find(\"tga\")\n",
    "    print(f\"start: {start}\")\n",
    "    print(f\"stop: {stop}\")\n",
    "    protein_seq = str(Seq(seq[start:stop]).translate())\n",
    "    return seq[0:start-1] + protein_seq + stop_symbol + seq[stop+3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 16\n",
      "stop: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages/Bio/Seq.py:2804: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TATAATAATAACGAA*GACGACAGTCGACAAGAAAAGCACCAGCTGTCCCCGCCACATACAAGTATATGAGAAGGGGACGCGGGAGAGCGCCGCGGGGGACCGACGCGCTATTGAGGGGGATGGGTACAAGCGGGGCGGGGAGGCCGGAGCTTTATCCAGGCCAATGAATGGCCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAG'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnap_tokenize(seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive tokenizer like this does not account for ORF-sensitivity and will simply find the first occurence of the stop codon, even if it follows the start codon right away. All sequences such as `atga` would then be truncated to a stop codon with Biopython translation API. So we need something smarter that chunks up the sequences into codons once the ORF is found. This is a good time to develop or pick up an ORF finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8518.86s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/chris/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: orffinder in /home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages (1.8)\n",
      "Requirement already satisfied: biopython>=1.79 in /home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages (from orffinder) (1.81)\n",
      "Requirement already satisfied: numpy in /home/chris/miniconda3/envs/enformer/lib/python3.10/site-packages (from biopython>=1.79->orffinder) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install orffinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 17, 'end': 173, 'frame': 2, 'sense': '+', 'length': 156, 'trailing': False, 'index': 1}, {'start': 173, 'end': 300, 'frame': 2, 'sense': '+', 'length': 127, 'trailing': True, 'index': 2}, {'start': 186, 'end': 300, 'frame': 3, 'sense': '+', 'length': 114, 'trailing': True, 'index': 3}, {'start': 169, 'end': 259, 'frame': 1, 'sense': '+', 'length': 90, 'trailing': False, 'index': 4}, {'start': 63, 'end': 1, 'frame': 2, 'sense': '-', 'length': 61, 'trailing': True, 'index': 5}, {'start': 259, 'end': 300, 'frame': 1, 'sense': '+', 'length': 41, 'trailing': True, 'index': 6}]\n"
     ]
    }
   ],
   "source": [
    "from Bio.SeqIO import SeqRecord\n",
    "from orffinder import orffinder\n",
    "\n",
    "orfs = orffinder.getORFs(SeqRecord(Seq(train_dataset['sequence'][0])), minimum_length=3)\n",
    "print(orfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "orf = orfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 17,\n",
       " 'end': 173,\n",
       " 'frame': 2,\n",
       " 'sense': '+',\n",
       " 'length': 156,\n",
       " 'trailing': False,\n",
       " 'index': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def dnap_tokenize(seq: str, orf_data: Dict[str, Any], stop_symbol: str = \"*\"):\n",
    "    start = orf_data['start']\n",
    "    stop = orf_data['end']\n",
    "    protein_seq = str(Seq(seq[start:stop]).translate())\n",
    "    return seq[0:start-1] + protein_seq + stop_symbol + seq[stop+3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TATAATAATAACGAAG*DDSRQEKHQLSPPHTSI*EGDAGERRGGPTRY*GGWVQAGRGGRSFIQANE*CCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAG'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnap_tokenize(seq, orf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's cool and all but these are all promoter sequences, so within those 300 bp here we would not expect any actual genes to be found.\n",
    "\n",
    "What we could do instead is append those sequences to existing gene sequences and encode specifically those gene sequences only using a DNA+P representation. We would be using synthetic data but it would be much easier to prepare than looking for specific genes in databases that are prepended by those promoters that Prom300 contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TATAATAATAACGAAGATGAGACGACAGTCGACAAGAAAAGCACCAGCTGTCCCCGCCACATACAAGTATATGAGAAGGGGACGCGGGAGAGCGCCGCGGGGGACCGACGCGCTATTGAGGGGGATGGGTACAAGCGGGGCGGGGAGGCCGGAGCTTTATCCAGGCCAATGAATGGCCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAG'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's develop a tiny function that will append randomly generated gene sequences to the sequences in the dataset that are marked as promoters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TATAATAATAACGAAGATGAGACGACAGTCGACAAGAAAAGCACCAGCTGTCCCCGCCACATACAAGTATATGAGAAGGGGACGCGGGAGAGCGCCGCGGGGGACCGACGCGCTATTGAGGGGGATGGGTACAAGCGGGGCGGGGAGGCCGGAGCTTTATCCAGGCCAATGAATGGCCACTTGCGATGCCCAATTGCACCAAGCTTGGAGCGCACACTCAACCCCTTCCCCAGCGGTATGCCAAAATTCACCGTCTGAATGGCGTTGGTGCAGGTCGGTACAGAGCTCTCCTGCGCCGAGGCAGGCAAGTGGGGCACCCGTATCCTTTCCAACTTACAAGGGTCCCCGTTGTGCGCCAGAGGAAGTCACTTTATATCCGCGCAC'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from typing import Dict, Any, List\n",
    "import Bio.Data.CodonTable as CodonTable\n",
    "\n",
    "\n",
    "def append_gene_to_promoter(seq: str,\n",
    "                            gene_length: int,\n",
    "                            is_promoter_seq: bool = True,\n",
    "                            seed: int = 42,\n",
    "                            codon_table: CodonTable.CodonTable = CodonTable.standard_dna_table):\n",
    "    np.random.seed(seed)\n",
    "    num_codons = gene_length - 6 // 3  # -6 because we account for 1 start and 1 stop codon per gene appended\n",
    "    actual_gene_length = num_codons * 3  # recover if `gene_length` was not divisible by 3\n",
    "    if is_promoter_seq:\n",
    "        # 1. Generate a random in-ORF gene of length `gene_length`:\n",
    "        start = codon_table.start_codons[0]  # ATG\n",
    "        stop = np.random.choice(codon_table.stop_codons)  # randomly choose one\n",
    "        codon_seq = np.random.choice(list(codon_table.forward_table.keys()),\n",
    "                                     size=num_codons,\n",
    "                                     replace=True)\n",
    "        codon_seq = \"\".join(list(codon_seq))\n",
    "        # 2. Append the gene to the promoter sequence if the sequence is marked as a promoter:\n",
    "        return seq + start + codon_seq + stop\n",
    "    else:\n",
    "        # 1. Generate a random DNA sequence that does not start with an `ATG` codon\n",
    "        nucleotides = [\"A\", \"T\", \"G\", \"C\"]\n",
    "        rand_nucl_seq = np.random.choice(nucleotides,\n",
    "                                         size=actual_gene_length,\n",
    "                                         replace=True)\n",
    "        # 2. Append the padding sequence to the non-promoter sequence\n",
    "        rand_nucl_seq = \"\".join(list(rand_nucl_seq))\n",
    "        return seq + rand_nucl_seq\n",
    "\n",
    "\n",
    "def append_gene_to_dataset_record(seq: str, label: str, gene_length: int, seed: int = 42):\n",
    "    match label:\n",
    "        case 0:\n",
    "            return dict(sequence=append_gene_to_promoter(seq, gene_length, False, seed), label=label)\n",
    "        case 1:\n",
    "            return dict(sequence=append_gene_to_promoter(seq, gene_length, True, seed), label=label)\n",
    "        case _:\n",
    "            raise ValueError(f\"{label} is not a valid label for a binary classification task\")\n",
    "\n",
    "\n",
    "def append_gene_to_dataset_batch(batch: Dict[str, List[Any]], gene_length: int, seed: int = 42):\n",
    "    seqs = batch[\"sequence\"]\n",
    "    labels = batch[\"label\"]\n",
    "    out = dict(sequence=[], label=[])\n",
    "    for seq, label in zip(seqs, labels):\n",
    "        updated_record = append_gene_to_dataset_record(seq, label, gene_length, seed)\n",
    "        out[\"sequence\"].append(updated_record[\"sequence\"])\n",
    "        out[\"label\"].append(updated_record[\"label\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "def append_genes_to_dataset(dataset_collection: DatasetDict, gene_length: int, seed: int = 42):\n",
    "    new_dataset_collection = DatasetDict()\n",
    "    for k in dataset_collection.keys():\n",
    "        # `batched=True` speeds things up by processing sequences in batches\n",
    "        new_dataset_collection[k] = dataset_collection[k].map(lambda batch: append_gene_to_dataset_batch(batch,\n",
    "                                                                                                         gene_length,\n",
    "                                                                                                         seed),\n",
    "                                                              batched=True)\n",
    "    return new_dataset_collection\n",
    "\n",
    "\n",
    "append_gene_to_promoter(seq, 30, True)\n",
    "append_gene_to_promoter(seq, 30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chris/.cache/huggingface/datasets/csv/default-68212c3a0ebc43dc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-2e209b8db1408bd3.arrow\n",
      "Loading cached processed dataset at /home/chris/.cache/huggingface/datasets/csv/default-68212c3a0ebc43dc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-14fa5a68aaa0ef51.arrow\n",
      "Loading cached processed dataset at /home/chris/.cache/huggingface/datasets/csv/default-68212c3a0ebc43dc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-286709a65c82b55a.arrow\n"
     ]
    }
   ],
   "source": [
    "promoter_dataset_with_genes_appended = append_genes_to_dataset(promoter_dataset, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 47356\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 5920\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sequence', 'label'],\n",
       "        num_rows: 5920\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promoter_dataset_with_genes_appended"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: because each sequence in the Prom300 dataset was exactly 300bp-long, we now know that ORFs for those samples that _were_ marked as actual promoters are in fact fake genes that we appended. We can **prepare a tokenizer for DNA+P representation with this in mind** considering only the protein-encoding part (over the 300bp boundary) as our starting point for the ORF. So we need to consider two things for the tokenizer:\n",
    "1. Start translating the ORF at 301bp mark (that will be string idx `300`).\n",
    "2. Only the sequences labeled as containing promoters should have the tokenizer applied to them.\n",
    "\n",
    "**Very important**: for the test set the DNA+P tokenizer **must not** convert the sequences to DNA+P representation. They should stay in their original form so that we can check whether _the model_ learned to pick up on those promoter sequences better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<CLS>', '<ACGTGT>', '<A>', '<C>', '<N>', '<TGCACG>', '<G>', '<A>', '<N>', '<CGACTA>', '<GTCTGA>']\n",
      "['<CLS>', '<ACGTGT>', '<A>', '<C>', '<N>', '<TGCACG>', '<G>', '<A>', '<N>', '<CGACTA>', '<GTCTGA>']\n",
      "True\n",
      "['<CLS>', ('<ACGTGT>', 0, 6), ('<A>', 0, 1), ('<C>', 1, 2), ('<N>', 2, 3), ('<TGCACG>', 9, 15), ('<G>', 0, 1), ('<A>', 1, 2), ('<N>', 2, 3), ('<CGACTA>', 18, 24), ('<GTCTGA>', 24, 30), ('<MetAsp>', 30, 36), ('<AspAsp>', 36, 42), ('<Asp*>', 42, 48)]\n",
      "['<CLS>', ('<ACGTGT>', 0, 6), ('<A>', 0, 1), ('<C>', 1, 2), ('<N>', 2, 3), ('<TGCACG>', 9, 15), ('<G>', 0, 1), ('<A>', 1, 2), ('<N>', 2, 3), ('<CGACTA>', 18, 24), ('<GTCTGA>', 24, 30), ('<MD>', 30, 36), ('<DD>', 36, 42), ('<D*>', 42, 48)]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    NormalizedString,\n",
    "    PreTokenizedString\n",
    ")\n",
    "from itertools import accumulate, islice\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Literal, Optional\n",
    "from Bio.Data.CodonTable import standard_dna_table\n",
    "\n",
    "protein_alphabet_map = {\n",
    "    \"A\": \"Ala\",\n",
    "    \"C\": \"Cys\",\n",
    "    \"D\": \"Asp\",\n",
    "    \"E\": \"Glu\",\n",
    "    \"F\": \"Phe\",\n",
    "    \"G\": \"Gly\",\n",
    "    \"H\": \"His\",\n",
    "    \"I\": \"Ile\",\n",
    "    \"K\": \"Lys\",\n",
    "    \"L\": \"Leu\",\n",
    "    \"M\": \"Met\",\n",
    "    \"N\": \"Asp\",\n",
    "    \"P\": \"Pro\",\n",
    "    \"Q\": \"Gln\",\n",
    "    \"R\": \"Arg\",\n",
    "    \"S\": \"Ser\",\n",
    "    \"T\": \"Thr\",\n",
    "    \"V\": \"Val\",\n",
    "    \"W\": \"Trp\",\n",
    "    \"Y\": \"Tyr\",\n",
    "    \"*\": \"*\"\n",
    "}\n",
    "\n",
    "\n",
    "class Uppercase:\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def normalize(self, normalized: NormalizedString):\n",
    "        return normalized.uppercase()\n",
    "\n",
    "    def normalize_str(self, sequence: str):\n",
    "        return sequence.upper()\n",
    "\n",
    "\n",
    "def take_n(iterable: Iterable, n: int):\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        chunk = \"\".join(list(islice(iterator, n)))\n",
    "        if chunk is None:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "@dataclass\n",
    "class DNAPConfig:\n",
    "    start_translating_from_idx: int\n",
    "    _length: int = -1\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        # if self._length == -1:\n",
    "            # return -1\n",
    "        return self._length\n",
    "\n",
    "\n",
    "    def translate(self, seq: str):\n",
    "        return dnap_tokenize(seq, dict(start=self.start_translating_from_idx,\n",
    "                                       end=self.start_translating_from_idx + self.length))\n",
    "\n",
    "\n",
    "def _wrap_in_angle_brackets(seq: str, wrap_tokens: bool = True):\n",
    "    return seq if not wrap_tokens else \"<\" + seq + \">\"\n",
    "\n",
    "\n",
    "def _pre_tokenize(sequence: str,\n",
    "                  k: int,\n",
    "                  include_cls: bool,\n",
    "                  split_on_unknown_base: bool,\n",
    "                  wrap_tokens: bool,\n",
    "                  include_ranges: bool,\n",
    "                  representation: Literal[\"dna\", \"dnap\"],\n",
    "                  dnap_config: Optional[DNAPConfig] = None):\n",
    "    \n",
    "    def _with_ranges(seq: str, idx_start: int, idx_end: int):\n",
    "        subslice = seq\n",
    "        if include_ranges:\n",
    "            subslice = (seq, idx_start, idx_end)\n",
    "        return subslice\n",
    "\n",
    "    def _dnap_chunk_tokenize(seq: str, stop_symbol: str = \"*\"):\n",
    "        out = []\n",
    "        n = 3\n",
    "        chunks = [seq[i:i+n] for i in range(0, len(seq), n)]\n",
    "        lookup_table = deepcopy(standard_dna_table.forward_table)\n",
    "        lookup_table.update({k: stop_symbol for k in standard_dna_table.stop_codons})\n",
    "        # Replace single-letter identifiers with the 3-letter ones:\n",
    "        three_letter_lookup_table = {k: protein_alphabet_map[v] for k, v in lookup_table.items()}\n",
    "        for codon in chunks:\n",
    "            try:\n",
    "                aa = three_letter_lookup_table[codon]\n",
    "                out.append(aa)\n",
    "            except KeyError:\n",
    "                out.append(codon)\n",
    "        return \"\".join(out)\n",
    "\n",
    "    if dnap_config is None and representation == \"dnap\":\n",
    "        raise ValueError(\"Missing DNAP representation config!\")\n",
    "\n",
    "    i = 0\n",
    "    j = k\n",
    "    n = len(str(sequence))\n",
    "    slices = [] if not include_cls else [\"<CLS>\"]\n",
    "    # for seq_slice in take_n(sequence, k):\n",
    "    while j <= n:\n",
    "        seq_slice = sequence[i:j]\n",
    "        if \"N\" in seq_slice and split_on_unknown_base:\n",
    "            idx_of_n = seq_slice.find(\"N\")\n",
    "            l = 0\n",
    "            subslices = []\n",
    "            while l != idx_of_n:\n",
    "                subslice = _with_ranges(_wrap_in_angle_brackets(seq_slice[l], wrap_tokens), l, l + 1)\n",
    "                subslices.append(subslice)  # split preceding into single bases\n",
    "                l += 1\n",
    "            subslice = _with_ranges(_wrap_in_angle_brackets(seq_slice[idx_of_n], wrap_tokens), idx_of_n, idx_of_n + 1)\n",
    "            subslices.append(subslice)  # add `N` itself\n",
    "            slices.extend(subslices)  # add the split-up slices to the main list\n",
    "            i = i + k - idx_of_n - 1\n",
    "            j = j + k - idx_of_n - 1\n",
    "        else:\n",
    "            if representation == \"dnap\" and i >= dnap_config.start_translating_from_idx:\n",
    "                slice_ = _with_ranges(_wrap_in_angle_brackets(_dnap_chunk_tokenize(seq_slice), wrap_tokens), i, j)\n",
    "            else:\n",
    "                slice_ = _with_ranges(_wrap_in_angle_brackets(seq_slice, wrap_tokens), i, j)\n",
    "            slices.append(slice_)\n",
    "            i += k\n",
    "            j += k\n",
    "    return slices\n",
    "\n",
    "\n",
    "# We use identical tokenization strategy to Nucleotide Transformer\n",
    "class KMerDNAPreTokenizer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 k: int = 6,\n",
    "                 include_cls: bool = True,\n",
    "                 wrap_tokens: bool = True,\n",
    "                 split_on_unknown_base: bool = True,\n",
    "                 include_ranges: bool = True) -> None:\n",
    "        \"\"\"Args:\n",
    "            `k` (int): how many bases per token\n",
    "            `include_cls` (bool): whether to include the starting `<CLS>` token or not\n",
    "            `wrap_tokens` (bool): whether to wrap each token in `<>` angle brackets\n",
    "            `split_on_unknown_base` (bool): if `True` then each `N` in the sequence will\n",
    "                                            be treated as a separate token and all preceding\n",
    "                                            bases that don't align to `k` are split up as\n",
    "                                            separate tokens\n",
    "            `include_ranges` (bool): whether token slice ranges should be returned in tuples\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.include_cls = include_cls\n",
    "        self.wrap_tokens = wrap_tokens\n",
    "        self.split_on_unknown_base = split_on_unknown_base\n",
    "        self.include_ranges = include_ranges\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        # Note: `pre_tokenize` acts in-place on `pretok`, so we should not return anything here\n",
    "        pretok.split(lambda idx, x: _pre_tokenize(x,\n",
    "                                                  self.k,\n",
    "                                                  self.include_cls,\n",
    "                                                  self.split_on_unknown_base,\n",
    "                                                  self.wrap_tokens,\n",
    "                                                  self.include_ranges,\n",
    "                                                  representation=\"dna\"))\n",
    "\n",
    "    def pre_tokenize_str(self, sequence: str):\n",
    "        return _pre_tokenize(sequence,\n",
    "                             self.k,\n",
    "                             self.include_cls,\n",
    "                             self.split_on_unknown_base,\n",
    "                             self.wrap_tokens,\n",
    "                             self.include_ranges,\n",
    "                             representation=\"dna\")\n",
    "\n",
    "\n",
    "class KMerDNAPPreTokenizer(KMerDNAPreTokenizer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 k: int = 6,\n",
    "                 include_cls: bool = True,\n",
    "                 wrap_tokens: bool = True,\n",
    "                 split_on_unknown_base: bool = True,\n",
    "                 include_ranges: bool = True,\n",
    "                 start_idx: int = 301) -> None:\n",
    "        self.start_idx = start_idx\n",
    "        super().__init__(k,\n",
    "                         include_cls,\n",
    "                         wrap_tokens,\n",
    "                         split_on_unknown_base,\n",
    "                         include_ranges)\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(lambda idx, x: _pre_tokenize(x,\n",
    "                                                  self.k,\n",
    "                                                  self.include_cls,\n",
    "                                                  self.split_on_unknown_base,\n",
    "                                                  self.wrap_tokens,\n",
    "                                                  self.include_ranges,\n",
    "                                                  representation=\"dnap\",\n",
    "                                                  dnap_config=DNAPConfig(self.start_idx)))\n",
    "\n",
    "    def pre_tokenize_str(self, sequence: str):\n",
    "        return _pre_tokenize(sequence,\n",
    "                             self.k,\n",
    "                             self.include_cls,\n",
    "                             self.split_on_unknown_base,\n",
    "                             self.wrap_tokens,\n",
    "                             self.include_ranges,\n",
    "                             representation=\"dnap\",\n",
    "                             dnap_config=DNAPConfig(self.start_idx))\n",
    "\n",
    "\n",
    "# normalizer = normalizers.Lowercase()\n",
    "# normalizer = normalizers.Normalizer.custom(Uppercase())\n",
    "# pre_tokenizer = pre_tokenizers.PreTokenizer.custom(KMerDNAPreTokenizer(6))\n",
    "# tokenizer = Tokenizer()\n",
    "# normalizer = Uppercase()\n",
    "pre_tokenizer = KMerDNAPreTokenizer(6, include_ranges=False)\n",
    "\n",
    "# we're using the same example as the Nucleotide Transformer to test:\n",
    "original = \"ACGTGTACNTGCACGGANCGACTAGTCTGA\"\n",
    "actual = pre_tokenizer.pre_tokenize_str(original)\n",
    "expected = [\"<CLS>\",\"<ACGTGT>\",\"<A>\",\"<C>\",\"<N>\",\"<TGCACG>\",\"<G>\",\"<A>\",\"<N>\",\"<CGACTA>\",\"<GTCTGA>\"]\n",
    "print(actual)\n",
    "print(expected)\n",
    "\n",
    "# compare the two lists:\n",
    "print(all(accumulate(map(lambda x: x[0] == x[1], zip(actual, expected, strict=True)), lambda x, y: x and y)))\n",
    "\n",
    "# then instantiate the other pre-tokenizer:\n",
    "pre_tokenizer = KMerDNAPPreTokenizer(6, start_idx=30)\n",
    "original = \"ACGTGTACNTGCACGGANCGACTAGTCTGAATGGATGATGATGATTGA\"\n",
    "actual = pre_tokenizer.pre_tokenize_str(original)\n",
    "expected = ['<CLS>', ('<ACGTGT>', 0, 6), ('<A>', 0, 1), ('<C>', 1, 2), ('<N>', 2, 3), ('<TGCACG>', 9, 15), ('<G>', 0, 1), ('<A>', 1, 2), ('<N>', 2, 3), ('<CGACTA>', 18, 24), ('<GTCTGA>', 24, 30), ('<MD>', 30, 36), ('<DD>', 36, 42), ('<D*>', 42, 48)]\n",
    "print(actual)\n",
    "print(expected)\n",
    "print(all(accumulate(map(lambda x: x[0] == x[1], zip(actual, expected, strict=True)), lambda x, y: x and y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very important**: for the test set the DNA+P tokenizer **must not** convert the sequences to DNA+P representation. They should stay in their original form so that we can check whether _the model_ learned to pick up on those promoter sequences better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = promoter_dataset_with_genes_appended[\"train\"]\n",
    "val_dataset = promoter_dataset_with_genes_appended[\"val\"]\n",
    "test_dataset = promoter_dataset_with_genes_appended[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from tokenizers.models import Unigram\n",
    "from pathlib import Path\n",
    "\n",
    "vocab_ = [\"A\", \"T\", \"G\", \"C\"]\n",
    "vocab_ += list(protein_alphabet_map.values())\n",
    "vocab = list(zip(vocab_, [1 for i in range(len(vocab_))]))\n",
    "\n",
    "pre_tokenizer = pre_tokenizers.PreTokenizer.custom(KMerDNAPreTokenizer(6))\n",
    "tokenizer = Tokenizer(Unigram(vocab=vocab))\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "tokenizer.encode(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loader = lambda: AutoModelForSequenceClassification.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", num_labels=2)\n",
    "model_save_dir = Path(\"results/model_nucleotide_transformer\")\n",
    "training_args = TrainingArguments(model_save_dir,\n",
    "                                  evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "from transformers import EvalPrediction, PreTrainedTokenizerFast\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "import evaluate\n",
    "\n",
    "def make_metrics_func(*dataset_load_args):\n",
    "    def compute_metrics(eval_pred: EvalPrediction):\n",
    "        accuracy = evaluate.load(\"accuracy\")\n",
    "        logits, labels = eval_pred\n",
    "        pred_class = np.argmax(logits, axis=-1)  # take the max-scoring logit as the predicted class ID\n",
    "        return accuracy.compute(predictions=pred_class,\n",
    "                                references=labels)\n",
    "    return compute_metrics\n",
    "\n",
    "compute_metrics = make_metrics_func()\n",
    "\n",
    "model_save_dir = Path(\"results/model_nucleotide_transformer\")\n",
    "training_args = TrainingArguments(model_save_dir,\n",
    "                                    evaluation_strategy=\"epoch\")\n",
    "\n",
    "\n",
    "def tokenize(dataset: Dataset):\n",
    "    fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "    fast_tokenizer._tokenizer.pre_tokenizer=PreTokenizer.custom(KMerDNAPreTokenizer(6))\n",
    "    fast_tokenizer.save(\"tok.json\")\n",
    "    fast_tokenizer = PreTrainedTokenizerFast.from_file(\"tok.json\")\n",
    "    return fast_tokenizer(dataset[\"sequence\"], padding=True)\n",
    "\n",
    "\n",
    "def training_pipeline(tokenizer,\n",
    "                      model,\n",
    "                      training_args,\n",
    "                      train_dataset,\n",
    "                      val_dataset):\n",
    "\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    tokenized_val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/chris/.cache/huggingface/hub/models--InstaDeepAI--nucleotide-transformer-500m-human-ref/snapshots/789373eb1adf0dda569e7fe55e4fc2adb15593bc/config.json\n",
      "Model config EsmConfig {\n",
      "  \"_name_or_path\": \"InstaDeepAI/nucleotide-transformer-500m-human-ref\",\n",
      "  \"architectures\": [\n",
      "    \"EsmForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1280,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5120,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token_id\": 2,\n",
      "  \"max_position_embeddings\": 1002,\n",
      "  \"model_type\": \"esm\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 4105\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/chris/.cache/huggingface/hub/models--InstaDeepAI--nucleotide-transformer-500m-human-ref/snapshots/789373eb1adf0dda569e7fe55e4fc2adb15593bc/pytorch_model.bin\n",
      "Some weights of the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref were not used when initializing EsmForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                     \r"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error while attempting to pickle Tokenizer: Custom PreTokenizer cannot be serialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[39m=\u001b[39m training_pipeline(tokenizer, model_loader(), training_args, train_dataset, val_dataset)\n\u001b[1;32m      2\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[48], line 38\u001b[0m, in \u001b[0;36mtraining_pipeline\u001b[0;34m(tokenizer, model, training_args, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_pipeline\u001b[39m(tokenizer,\n\u001b[1;32m     33\u001b[0m                       model,\n\u001b[1;32m     34\u001b[0m                       training_args,\n\u001b[1;32m     35\u001b[0m                       train_dataset,\n\u001b[1;32m     36\u001b[0m                       val_dataset):\n\u001b[0;32m---> 38\u001b[0m     tokenized_train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mmap(tokenize, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     39\u001b[0m     tokenized_val_dataset \u001b[39m=\u001b[39m val_dataset\u001b[39m.\u001b[39mmap(tokenize, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     42\u001b[0m         model,\n\u001b[1;32m     43\u001b[0m         training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     48\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:3087\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3081\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3082\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3085\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3087\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3088\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3089\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:3463\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3459\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3460\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3461\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3462\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3463\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3464\u001b[0m         batch,\n\u001b[1;32m   3465\u001b[0m         indices,\n\u001b[1;32m   3466\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3467\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3468\u001b[0m     )\n\u001b[1;32m   3469\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3470\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3471\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3472\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:3344\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3343\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3344\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3346\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3347\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3348\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[48], line 25\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(dataset: Dataset):\n\u001b[0;32m---> 25\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m PreTrainedTokenizerFast(tokenizer_object\u001b[39m=\u001b[39;49mtokenizer)\n\u001b[1;32m     26\u001b[0m     fast_tokenizer\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mpre_tokenizer\u001b[39m=\u001b[39mPreTokenizer\u001b[39m.\u001b[39mcustom(KMerDNAPreTokenizer(\u001b[39m6\u001b[39m))\n\u001b[1;32m     27\u001b[0m     fast_tokenizer\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mtok.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:108\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot instantiate this tokenizer from a slow version. If it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms based on sentencepiece, make sure you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhave sentencepiece installed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_object \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(tokenizer_object)\n\u001b[1;32m    109\u001b[0m \u001b[39melif\u001b[39;00m fast_tokenizer_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m from_slow:\n\u001b[1;32m    110\u001b[0m     \u001b[39m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m TokenizerFast\u001b[39m.\u001b[39mfrom_file(fast_tokenizer_file)\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer/lib/python3.10/copy.py:161\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    159\u001b[0m reductor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__reduce_ex__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m reductor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     rv \u001b[39m=\u001b[39m reductor(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     reductor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__reduce__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mException\u001b[0m: Error while attempting to pickle Tokenizer: Custom PreTokenizer cannot be serialized"
     ]
    }
   ],
   "source": [
    "trainer = training_pipeline(tokenizer, model_loader(), training_args, train_dataset, val_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
